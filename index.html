<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="PhyPlan: Generalizable and Rapid Physical Task Planning with Physics-Informed Skill Networks for Robot Manipulators">
  <meta name="keywords" content="PhyPlan, PhyPlan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PhyPlan</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dnerf"></span>Zero-Shot Temporal Reasoning over Object Interaction for Robot Instruction Following
              </h1>
              <h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);">......</h3>
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Riya Arora<sup>1*</sup>,</span>
                <span class="author-block">
                  Niveditha Narendranath<sup>1*</sup>,</span>
                <span class="author-block">
                  Aman Tambi<sup>1</sup>,</span>
                <span class="author-block">
                  Sandeep S. Zachariah<sup>1</sup>,</span>
                <span class="author-block">
                  Souvik Chakraborty<sup>1</sup>,</span>
                <span class="author-block">
                  Rohan Paul<sup>1</sup></span>
                
                
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <!-- <span class="author-block"><sup>1</sup>Work primarily done when at IIT Delhi, </span>-->
                <span class="author-block"><sup>1</sup>Affiliated with IIT Delhi, </span>
                <span class="author-block"><sup>{*}</sup>Indicate equal contributions</span>
              </div>

              <!-- <div class="column has-text-centered">
                <div class="publication-links"> -->
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="http://arxiv.org/abs/2402.15767" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmgQnyyezjnUCMDo3N?e=54ufNt"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/phyplan/PhyPlan"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmcmRXYJJINf42I20?e=XhDGne"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                <!-- </div>
              </div> -->
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  <hr>

  <div class="container is-max-desktop has-text-centered">
    <h2 class="subtitle" , style="max-width: 90%; padding-left: 10%;">
      Leveraging reasoning capabilities of Large Vision Language Models, Large Language Models, and Vision Language Models, to perform higher-order reasoning is a novel approach to perform zero-shot learning required for effective physical Human-Robot Interaction
    </h2>
  </div>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <h2 class="title has-text-centered is-3">Abstract</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          Robust Robot Instruction Following mandates robots to possess both spatial and temporal reasoning capabilities within their environment. While current methodologies have advanced robots' semantic and spatial comprehension, temporal reasoning remains deficient. In tasks requiring action execution, robots are expected to draw upon past observations to aid humans effectively, necessitating a human-level semantic understanding and memory. This paper highlights the challenge and proposes solutions essential for enhancing robots' ability to recall sequences of actions, crucial for assisting humans in diverse scenarios. Figure 1 exemplifies the expected memory-dependent action sequences.        </p>
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/TR_Pipeline.drawio.png"
              style="border-radius: 30px; border: 2px solid #555555; background-color: azure;" />
          </div>
          <!-- Text Column -->
          </div>
        </div>
        <!-- Subtitle Section -->
        <!-- <div class="has-text-justified">
          <h2 class="subtitle">
            <span class="dnerf">PhyPlan</span> is a novel physics-informed planning framework based on accelerated
            learning of Physical Reasoning Tasks using Physics Informed Skill Networks
          </h2>
        </div> -->
      

      <div class="olumns is-centered is-vcentered has-text-justified">
        <p>
          <b>Temporal Reasoning Pipeline:</b> Initially, a parser divides the instruction into two primary components: the past observation segment and the future action to be undertaken by the robot. Although Large Vision-Language Models (LVLMs) exhibit proficiency in identifying desired objects within video streams, they often lack the ability to ground these identifications. Thus, it becomes imperative to segment our reasoning pipeline into two distinct functions: an object identification function and an object grounding function. While LVLMs excel at object identification, they may not consistently provide accurate spatial properties of the identified objects, posing a challenge within our pipeline. To circumvent this limitation, we furnish the LVLM with a description of the initial scene, denoted as enabling it to incorporate aspects of object manipulation relative to the static setting. Subsequently, the LVLM furnishes a description of past actions and observations gleaned from the input video, which comprises a collection of frames. 
          The LVLM's output is then scrutinized to extract the object of interest along with its corresponding spatial properties. These extracted features are then fed into a phase grounding model which furnishes the coordinates of the objects necessitating manipulation by the robot. In summary, the entire reasoning process is compartmentalized into three distinct components: initial scene comprehension, object interaction recognition, and object grounding.
        </p>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Curated Dataset</h2>
      <div class="has-text-justified">
        <p>
          To evaluate the pipeline, we formulated a dataset of 30+ videos (mean length 6 seconds). The dataset consists of a variety of scenarios such as single action - single interaction, single action - multiple interactions, and multiple actions - multiple interactions, with similar and distinct objects in the scene.

        </p>
        <br>
      </div>
      <hr>
      <div class="columns is-multiline" style="justify-content: center;">
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1i1o-place_cloth.gif" alt="GIF 1" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Single Interaction - Similar Objects</h3>
          <!-- <p style="text-align: justify;">Skill network learns to determine the displacement and velocity of a sliding
            box with given initial velocity at any time queried on a rough plane</p> -->
        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1imo-place.gif" alt="GIF 2"
              style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Multiple Interactions - Distinct Objects</h3>
          <!-- <p style="text-align: justify;">Skill network learns to determine the location and velocity of ball thrown
            with
            given initial angle and velocity at any time queried.</p> -->
        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1a2i1o-pour_cup.gif" alt="GIF 3" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">One Action - Multiple Interactions - Similar Objects</h3>
          <!-- <p style="text-align: justify;">Skill network learns to determine angular position and angular velocity of
            pendulum at any time queried with given initial angular position.</p> -->
        </div>
        
      </div>
      <!-- Second Row of GIFs -->
      <div class="columns is-multiline" style="justify-content: center;">
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1imo.gif" alt="GIF 4" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Single Interaction - Distinct Objects</h3>
          <!-- <p style="text-align: justify;">Skill network learns to determine velocity of puck just after it gets hit by a
            swinging pendulum.</p> -->
        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/tmp_reason24.gif" alt="GIF 5" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Multiple Action - Multiple Interaction - Distinct Objects</h3>
          <!-- <p style="text-align: justify;">Skill network learns to determine velocity of ball just after it bounces a
            wedge.</p> -->
        </div>
      </div>

      <!-- <div class="columns is-centered is-vcentered"> -->
        <!-- Text Column -->
        <!-- <div class="column is-half has-text-justified">
          <p>
            <i><b>The skill learning model is based on a neural network that predicts the object's state during dynamic
                interaction continuously parameterised by time.</i></b> The figure on the side shows the predicted
            positions of the ball plotted against time in the Bounce Task. Such interactions can be simulated in a
            physics engine by using
            numerical integration schemes.
            However, since we aim to perform multi-step interactions, simulating outcomes during training is often
            intractable. Hence, we adopt a learning-based approach and learn a function which predicts the object's
            state during dynamic interaction continuously parameterised by time.
            For certain skills like swining, sliding and throwing we levarage the known governing physics equations and
            employ a physics-informed loss function in neural network to constrain the latent space, these are called as
            Physics-Informed Skill Networks.
            However, skills like bouncing and hitting are learnt directly from data because of complex and intractable
            physics.
          </p>
        </div> -->
        <!-- Image Column -->
        <!-- <div class="column is-half">
          <img src="./static/images/bounce_chain_0.5.png" width="1499" height="976"
            style="border-radius: 25px; border: 2px solid #000000;" />
        </div> -->
      <!-- </div> -->
      <!-- <div class="has-text-justified">
        <p>
          The skill learning model is based on a neural network that predicts the object's state during dynamic
          interaction continuously parameterised by time. Such interactions can be simulated in a physics engine by
          using numerical integration schemes.
          However, since we aim to perform multi-step interactions, simulating outcomes during training is often
          intractable. Hence, we adopt a learning-based approach and learn a function which predicts the object's
          state during dynamic interaction continuously parameterised by time.
          For certain skills like swining, sliding and throwing we levarage the known governing physics equations and
          employ a physics-informed loss function in neural network to constrain the latent space, these are called as
          Physics-Informed Skill Networks.
          However, skills like bouncing and hitting are learnt directly from data because of complex and intractable
          physics.
        </p>
      </div> -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Results</h2>
      <div id="tasks" , class="column has-text-justified">
        <p>
          The LLMs of our choice were GPT4[1] for both the parsing tasks, GPT-4v[2] for initial scene description, Video-LLaVA[3] for object interaction recognition and CogVLM[4] for semantic object grounding. GPT-4v returns accurate and detailed description of the initial scene.
          Combining this with Video-LLaVA's temporal understanding over videos, especially short duration ones, and CogVLM's state-of-the-art grounding abilities, we were able to identify objects of interest with ease. It was observed that providing the initial scene description facilitates the LVLM in accurately identifying objects that have been manipulated, especially in ambiguous scenarios like similar objects in a scene. This technique helps our pipeline to surpass the hallucinating tendency of LVLMs and prevents it from misidentifying the objects of interest. Further, it was discovered that our choice of semantic object grounding model, CogVLM, performs exceptionally well even when the LVLM returns marginally incorrect outputs. 
          This rectifies our overall pipeline even if one component fails to work entirely accurately.
        </p>
      </div>
      <hr>
      <div class="columns is-multiline">
        <div class="column is-centered is-half">
          <div class="content">
            <p class="has-text-justified"></p>
            <img src="static/images/pie_chart.png" alt="GIF 4" style="width: 500px; height: 300px; border-radius: 25px;">

            <p class="teaser has-text-justified">
              Success Rate for Similar and Distinct cases in our dataset</p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <!-- <p class="has-text-justified">Robot trains to use pendulum object present in the environment to slide the
              puck to the goal</p> -->
              <img src="static/images/WhatsApp Image 2024-03-15 at 7.18.18 PM.jpeg" alt="GIF 4" style="width: 500px; height: 315px; border-radius: 25px;">
            <!-- <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;
              <source src="./static/video/slide.mp4" type="video/mp4">
            </video> -->
            <p class="teaser has-text-justified">
              Success Rate of our Pipeline for our dataset
            </p>
          </div>
        </div>

        <!-- <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Bounce Task</h3>
            <p class="has-text-justified">Robot trains to use wedge object present in the environment to make the ball
              reach the goal</p>
            <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;">
              <source src="./static/video/bounce.mp4" type="video/mp4">
            </video>
            <p class="teaser has-text-justified">
              In the above five trials, the robot places the wedge at the correct location with the proper orientation,
              throwing the ball from the proper height, accomplishing the ball reaching the goal.
            </p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Bridge Task</h3>
            <p class="has-text-justified">Robot trains to use the pendulum and bridge objects present in the environment
              to make the puck reach the
              goal</p>
            <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;">
              <source src="./static/video/bridge.mp4" type="video/mp4">
            </video>
            <p class="teaser has-text-justified">
              Over the shown trials, the robot learns to correctly align the pendulum so that the hitting plane is
              correctly aligned. Eventually, the robot effectively uses objects like the bridge present in the
              environment.
              <These, along with multiple physical skills involved in the task, highlight PhyPlan's adaptability to long-horizon tasks. -->
            </p>
          </div>
          <!-- OUR DEMO START -->
          <!-- <div class="content">
            <h3 class="title is-4 has-text-centered"><span class="dnerf">DQN-Adaptive</span></h3>
            <div class="columns is-centered is-vcentered">
              <!-- Image Column -->
              <!-- <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-1.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 1
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-4.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 4
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-11.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 11
                </p>
              </div>
            </div>
            <!-- Subtitle Section -->
            <!-- <p class="teaser has-text-justified">
              <span class="dnerf">DQN-Adaptive</span> (Baseline) executes actions in sequence while learning the difference
              in the predicted reward for an action and the actual reward. However, it does not use the bridge even after 11
              attempts which is needed to land the ball further closer to the goal.
            </p>
          </div> --> 
          <!-- <div class="content">
            <h3 class="title is-4 has-text-centered"><span class="dnerf">PhyPlan</span></h3>
            <div class="columns is-centered is-vcentered">
              <!-- Image Column -->
              <!-- <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_1.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 1
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_2.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 2
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_4.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 4
                </p>
              </div>
            </div> --> 
            <!-- Subtitle Section -->
            <!-- <p class="teaser has-text-justified">
              <span class="dnerf">PhyPlan</span> executes actions in sequence while learning the difference in the predicted
              reward for an action and the actual reward. It does not use the bridge in the first attempt because
              of errors in prediction. However, it quickly realises the need of the bridge in the second attempt. Further,
              it chooses appropriate actions to land the ball in the goal in just the fourth attempt. Note that the robot
              learns to use the bridge effectively; a physical reasoning task reported earlier <a
                href="https://arxiv.org/abs/1907.09620" class="external-link is-normal">[Allen et al., 2020]</a> to be
              challenging to learn for model-free methods, highlighting PhyPlan’s adaptability to long-horizon tasks.
            </p> -->
          <!-- </div> -->
          <!-- OUR DEMO END-->
        </div> 
      </div>
    </div>
    
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Conclusion and Future Work</h2>
      <div class="column has-text-justified">
        <p>
          It was found that GPT-4v returns accurate and detailed description of the initial scene. 
          Combining this with Video-LLaVA's temporal understanding over videos, especially short duration ones, and CogVLM's state-of-the-art grounding abilities, we were able to identify objects of interest with ease. It was observed that providing the initial scene description facilitates the LVLM in accurately identifying objects that have been manipulated, especially in ambiguous scenarios like similar objects in a scene. This technique helps our pipeline to surpass the hallucinating tendency of LVLMs and prevents it from misidentifying the objects of interest. Further, it was discovered that our choice of semantic object grounding model, CogVLM, performs exceptionally well even when the LVLM returns marginally incorrect outputs. 
          This rectifies our overall pipeline even if one component fails to work entirely accurately.
        </p>
      </div>
      <hr>

      
    </div>
  </section>

  <hr>

  <!-- <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [Allen et al., 2020] Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum.
        Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning.
        Proceedings of the National Academy of Sciences, 117(47):29302–29310, 2020.</code></pre>
      <pre><code>2. [Bakhtin et al., 2019] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick.
        Phyre: A new benchmark for physical reasoning.
        Advances in Neural Information Processing Systems, 32,484 2019.</code></pre>
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{phyplan2024,
      title     = {PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators},
      author    = {Vagadia, Harshil and Chopra, Mudit and Barnawal, Abhinav and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
