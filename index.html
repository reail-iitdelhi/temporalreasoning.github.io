<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="G2TRIF - Generalized Grounded Temporal Reasoning for Robot
    Instruction Following">
  <meta name="keywords" content="Temporal Reasoning, Zero-Shot, Robot Instruction Following ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Temporal Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dnerf"></span>G<sup>2</sup>TR - Generalized Grounded Temporal Reasoning for Robot
                Instruction Following via Coupled Pre-trained Models
              </h1>
              <h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);"></h3>
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Riya Arora<sup>1*</sup>,</span>
                <span class="author-block">
                  Niveditha Narendranath<sup>1*</sup>,</span>
                  
                <span class="author-block">
                  Aman Tambi<sup>1+</sup>,</span>
                  <span class="author-block">
                    Sandeep S. Zachariah<sup>1+</sup>,</span>
                
                <br>
                <span class="author-block">
                  Souvik Chakraborty<sup>1</sup>,</span>
                <span class="author-block">
                  Rohan Paul<sup>1</sup></span>
                
                
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <!-- <span class="author-block"><sup>1</sup>Work primarily done when at IIT Delhi, </span>-->
                <span class="author-block"><sup>1</sup>Affiliated with IIT Delhi, </span>
                <span class="author-block"><sup>{*,+}</sup>Indicate equal contributions</span>
              </div>

              <!-- <div class="column has-text-centered">
                <div class="publication-links"> -->
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="http://arxiv.org/abs/2402.15767" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmgQnyyezjnUCMDo3N?e=54ufNt"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/phyplan/PhyPlan"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmcmRXYJJINf42I20?e=XhDGne"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                <!-- </div>
              </div> -->
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  <hr>
<!-- 
  <div class="container is-max-desktop has-text-centered">
    <h2 class="subtitle" , style="max-width: 90%; padding-left: 10%;">
      [A short abstract of the paper goes here.]
    </h2>
  </div> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Introduction. -->
      <h2 class="title has-text-centered is-3">Introduction</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          <!-- Robust Robot Instruction Following mandates robots to possess  spatial and temporal reasoning capabilities within their environment. While current methodologies have advanced robots' semantic and spatial comprehension, temporal reasoning remains deficient. In tasks requiring action execution, robots are expected to draw upon past observations to aid humans effectively, necessitating a human-level semantic understanding and memory. This paper highlights the challenge and proposes solutions essential for enhancing robots' ability to recall sequences of actions, crucial for assisting humans in diverse scenarios. Figure 1 exemplifies the expected memory-dependent action sequences.        </p> -->
          Consider the scenario where a human cleans a
          table and a robot observing the scene is instructed with the task
          “Remove the cloth using which I wiped the table”. This requires
          the robot to first recognize the need for temporal reasoning
          about past events. The robot must then identify the relevant
          past object interaction, ground the object of interest in the
          present scene, and execute the task according to the human’s
          instruction. While previous efforts have enabled temporal reasoning in robots, 
          they typically rely on supervised learning with
          hand-crafted features. G<sup>2</sup>TR factorizes the grounded temporal
          reasoning problem as (i) realizing the problem through human-
          instruction, (ii) reasoning over the past for candidate interval,
          (iii) spatial reasoning over the interval for grounding, and (iv)
          reacquiring object post initial-grounding. We propose address-
          ing this challenge by leveraging the abilities of pre-trained
          Multimodel Vision-Language Models (LVLMs) To evaluate our
          framework, we release a robust dataset comprising 155 video-
          instruction pairs involving temporal, spatial, and linguistic
          complexity. Our approach has an average accuracy of 70.10%.
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Technical Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;>
          <!-- Image Column -->
          <div>
            <img src="./static/images/pipeline_diagram_v2.drawio.png"
              style="border-radius: 30px; border: 2px solid #555555; background-color: azure;" />
          </div>
          <!-- Text Column -->
          </div>
        </div>
        <!-- Subtitle Section -->
        <!-- <div class="has-text-justified">
          <h2 class="subtitle">
            <span class="dnerf">PhyPlan</span> is a novel physics-informed planning framework based on accelerated
            learning of Physical Reasoning Tasks using Physics Informed Skill Networks
          </h2>
        </div> -->
      

      <div class="columns is-centered is-vcentered has-text-justified">
        <p>
          <b>Temporal Reasoning Pipeline:</b><br>
          Our method factorize the
          grounded temporal reasoning task as (i) interpreting the
          language instruction as two distinct components: the past
          context and the required robot action, (ii) reasoning of a
          video (or image sequence) over time to localize candidate interval 
          with requisite interaction (iii) performing fine-grained
          spatial reasoning on this interval to ground target object (iv)
          re-acquiring this object post interaction.

          <br><br>
          (i) <b>Temporal Parsing</b>: The parser formlulates the two key questions:
          a temporal question that determines "when" the described interaction occurs, 
          and an object-identification question that specifiies "what" objects 
          is involved in the interaction.
          <br>
          (ii) <b>Event Localization</b>: This modules takes in the entire video 
          and the temporal question from the previous
          module, and returns the specific time instant 
          at which the specified interaction occurs.
          <br> 
          (iii) <b>Grounding Object of Interest</b>: The purpose of this module 
          is to identify the object with which the interaction occurred. This module
          operates in three steps: (a) The input is sent to a Target
          Identifier that extracts the class
          of the intended object that needs to be grounded. (b) A class-
          based detection returns the bounding box coordinates of all
          objects belonging to that class in the form of visual options.
          (c) The Target Identifier then refines the identification process using the 
          visual options provided in step (b).
          <br>
          (iv) <b>Object Tracking</b>: This module tracks the objects till the end of the video 
          and returns the final location of the object.
        </p>

      </div>
      <!-- <div class="columns is-centered is-vcentered", padding-left: 7%;>
        <img src="./static/images/TR_Pipeline.drawio(3).png"
          style="border-radius: 30px; border: 2px solid #555555; background-color: azure;" />
      </div> -->

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Curated Dataset</h2>
      <div class="has-text-justified">
        <p>
          To assess the robot’s ability
to perform grounded temporal reasoning, we form an
evaluation corpus of 155 video-instruction pairs. The data
set was collected in a table-top setting with a Franka Emika
Robot Manipulator observing the scene via a eye-in-hand
RGB-D camera. A total of 15 objects representating
common household and healthcare items such as cups,
bottles, medicines, fruits, notebooks, markers, handkerchiefs
etc. were used. A total of 8 participants performed
interactions such as pouring, placing, picking, stacking,
replacing, wiping, dropping, repositioning, swapping etc.
with 3 − 6 objects in each scene. The human participants
also provided natural language instructions for the robot
referencing to interactions and objects in the preceding
interactions in the workspace. This resulted in a corpus
of 155 video instruction pairs (each between 6 − 30
seconds). The instructions and interactions expressed natural
diversity of spatial and temporal reasoning complexity.
For detailed analysis, the evaluation corpus is bifurcated
as: (i) single/multi hop temporal reasoning (56 : 99)
(ii) simple/complex spatial grounding (98 : 57), (iii)
single/multiple interactions (62 : 93) and (iv) partial or full
observation of the referred object in the video (36 : 119),
see (Fig. 3). 
        </p>
        <br>
      </div>
      <hr>
      <div class="columns is-centered is-vcentered" , padding-left: 7%;>
        <!-- Image Column -->
        <div>
          <img src="./static/images/dataset_classification.drawio.png"
            style="border-radius: 30px; border: 2px solid #555555; background-color: azure; scale: 80%;" />
        </div>
        <!-- Text Column -->
        </div>
      </div>
      <!-- <div class="columns is-multiline" style="justify-content: center;">
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1i1o-place_cloth.gif" alt="GIF 1" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Single Interaction - Similar Objects</h3>

        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1imo-place.gif" alt="GIF 2"
              style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Multiple Interactions - Distinct Objects</h3>

        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1a2i1o-pour_cup.gif" alt="GIF 3" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">One Action - Multiple Interactions - Similar Objects</h3>

        </div>
        
      </div>

      <div class="columns is-multiline" style="justify-content: center;">
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1imo.gif" alt="GIF 4" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Single Interaction - Distinct Objects</h3>

        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/tmp_reason24.gif" alt="GIF 5" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Multiple Action - Multiple Interaction - Distinct Objects</h3>

        </div>
      </div> -->

      <!-- <div class="columns is-centered is-vcentered"> -->
        <!-- Text Column -->
        <!-- <div class="column is-half has-text-justified">
          <p>
            <i><b>The skill learning model is based on a neural network that predicts the object's state during dynamic
                interaction continuously parameterised by time.</i></b> The figure on the side shows the predicted
            positions of the ball plotted against time in the Bounce Task. Such interactions can be simulated in a
            physics engine by using
            numerical integration schemes.
            However, since we aim to perform multi-step interactions, simulating outcomes during training is often
            intractable. Hence, we adopt a learning-based approach and learn a function which predicts the object's
            state during dynamic interaction continuously parameterised by time.
            For certain skills like swining, sliding and throwing we levarage the known governing physics equations and
            employ a physics-informed loss function in neural network to constrain the latent space, these are called as
            Physics-Informed Skill Networks.
            However, skills like bouncing and hitting are learnt directly from data because of complex and intractable
            physics.
          </p>
        </div> -->
        <!-- Image Column -->
        <!-- <div class="column is-half">
          <img src="./static/images/bounce_chain_0.5.png" width="1499" height="976"
            style="border-radius: 25px; border: 2px solid #000000;" />
        </div> -->
      <!-- </div> -->
      <!-- <div class="has-text-justified">
        <p>
          The skill learning model is based on a neural network that predicts the object's state during dynamic
          interaction continuously parameterised by time. Such interactions can be simulated in a physics engine by
          using numerical integration schemes.
          However, since we aim to perform multi-step interactions, simulating outcomes during training is often
          intractable. Hence, we adopt a learning-based approach and learn a function which predicts the object's
          state during dynamic interaction continuously parameterised by time.
          For certain skills like swining, sliding and throwing we levarage the known governing physics equations and
          employ a physics-informed loss function in neural network to constrain the latent space, these are called as
          Physics-Informed Skill Networks.
          However, skills like bouncing and hitting are learnt directly from data because of complex and intractable
          physics.
        </p>
      </div> -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;"> Video Demonstration</h2>
      <div class="columns is-centered is-vcentered"> 
        <Text Column>
        <div class="column has-text-justified">
          <p>
            <i><b>Demonstration 1: Video showing the robot executing the task for the instruction "pick the object that I just placed."</i></b> <br>
          
            The robot in scene is a Franka Emika Arm Manipulator. There is a table in front of the robot, on which various objects such as a pair of glasses, a cup, 
            and a small bottle are already kept and the robot is observing this scene with its camera. Then a person comes and places a new object (a banana) on the table. 
            Then an instruction "pick the object that I just placed" is given to the robot. 
          </p>
        </div> 
        <Image Column>
        <div class="column is-centered">
          <video id="matting-video" controls playsinline height="70%" width="70%" style="border-radius: 25px;">
            <source src="./static/video/video_demo.mp4" type="video/mp4">
          </video> 
          <div class="column has-text-justified">
            <p>
              <i><b>Demonstration 2: Video showing the robot responding to the question "where is the banana?"</i></b> <br>
              The robot in scene is a Franka Emika Arm Manipulator. There is a table in front of the robot, on which two books,two markers and a bag is placed.
              The robot is observing this scene with its camera. Then the robot is given the instruction "where is the marker that I just used". 
              The robot calls the G<sup>2</sup>TR pipeline to identify the object being referred and points
              to the bag inside which the marker was placed.
            </p>
          </div> 
          <Image Column>
          <div class="column is-centered">
            <video id="matting-video" controls playsinline height="70%" width="70%" style="border-radius: 25px;">
              <source src="./static/video/marker_pointing.mp4" type="video/mp4">
            </video> 
        </div> 
      </div> 
    </div>  
  </section>>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experimental Insights</h2>
      <div id="tasks" , class="column has-text-justified">
        <p>
          We have compared the performance of the proposed
          model G<sup>2</sup>TR with alternative approaches -Direct Temporal Visual Grounding (DTVG) and Refined Temporal Visual Grounding (RTVG).
           The proposed approach has better performance 
          than suggested alternate approaches by 26%, in terms
          of overall average grounding accuracy.
          We compared the state of the art LVLM and LLM models to realize various aspects of the proposed pipeline. We use GPT4 for both the parsing tasks, GPT-4V for initial scene description, Video-LLaVA for object interaction recognition and CogVLM for semantic object grounding. The experimentation of the entire end-to-end pipeline was carried out on a Franka Emika robot manipulator. The robot observes a person placing an object and later successfully executes the instruction, "robot, pick up the object that I just placed". 
          The figure below show the accuracy of instruction following for the scenarios partitioned as those containing identical vs. distinct objects on the scene. The overall success rate was obtained as 45.3%. We observe that LVLMs gave incorrect or ambiguous outputs when all the objects on the scene were identical while higher performance for scenarios with distinct objects. Results indicate the lack of robustness in contemporary LVLMs in identifying complex spatial properties.  
        </p>
      </div>
      <hr>
      <div class="columns is-multiline">
        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Component-Wise Analysis of Failures</h3>
            <img src="static/images/component_analysis_v4.drawio.png" alt="GIF 4" style="width: 600px; height: 280px; border-radius: 25px;">

            <p class="teaser has-text-justified">
              Component-Wise Analysis frequency of failure cases generated by each module</p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Failure Case Examples</h3>
            <!-- <p class="has-text-justified">Robot trains to use pendulum object present in the environment to slide the
              puck to the goal</p> -->
              <img src="static/images/failure_cases_v2.drawio.png" alt="GIF 4" style="width: 400px; height: 280px; border-radius: 25px;">
            <!-- <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;
              <source src="./static/video/slide.mp4" type="video/mp4">
            </video> -->
            <p class="teaser has-text-justified">
              Examples of Failure Cases displays instances of failure within G<sup>2</sup>TR framework.
            </p>
          </div>
        </div>

        <!-- <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Example - Failure Case 1</h3>
            <p class="has-text-justified">Scenario in which all objects are completely identical and the spatial properties get complex (objects are too close to each-other)</p>
            
              <img src="static/images/Untitled Diagram.drawio.png" alt="GIF 4" style="width: 500px; height: 315px; border-radius: 25px;">
            
            <p class="teaser has-text-justified">
              For instruction ”pick the object that was just placed”, VideoLLaVA outputs "The object that was placed was the cup in the center". Hence, the output of our proposed pipeline (via Video-LLAVA) (in red) deviates from the ground truth (in green) resulting in erronous temporal reasoning.
            </p>
          </div>
        </div> 

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Example - Failure Case 2</h3>
            <p class="has-text-justified"> Scenario in which two identical objects are placed among other distinct objects.</p>
            
              <img src="static/images/amb2.drawio.png" alt="GIF 4" style="width: 500px; height: 315px; border-radius: 25px;">
            </video>
            <p class="teaser has-text-justified">
              For instruction ”pick the object that was just placed”, VideoLLaVA outputs "The object that was placed was the controller near the person." Hence, the output of our proposed pipeline (via Video-LLAVA) (in red) deviates from the ground truth (in green) resulting in erronous temporal reasoning.
              <These, along with multiple physical skills involved in the task, highlight PhyPlan's adaptability to long-horizon tasks. -->
            <!-- </p>
          </div>  -->
          <!-- OUR DEMO START -->
          <!-- <div class="content">
            <h3 class="title is-4 has-text-centered"><span class="dnerf">DQN-Adaptive</span></h3>
            <div class="columns is-centered is-vcentered">
              <!-- Image Column -->
              <!-- <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-1.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 1
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-4.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 4
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-11.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 11
                </p>
              </div>
            </div>
            <!-- Subtitle Section -->
            <!-- <p class="teaser has-text-justified">
              <span class="dnerf">DQN-Adaptive</span> (Baseline) executes actions in sequence while learning the difference
              in the predicted reward for an action and the actual reward. However, it does not use the bridge even after 11
              attempts which is needed to land the ball further closer to the goal.
            </p>
          </div> --> 
          <!-- <div class="content">
            <h3 class="title is-4 has-text-centered"><span class="dnerf">PhyPlan</span></h3>
            <div class="columns is-centered is-vcentered">
              <!-- Image Column -->
              <!-- <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_1.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 1
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_2.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 2
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_4.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 4
                </p>
              </div>
            </div> --> 
            <!-- Subtitle Section -->
            <!-- <p class="teaser has-text-justified">
              <span class="dnerf">PhyPlan</span> executes actions in sequence while learning the difference in the predicted
              reward for an action and the actual reward. It does not use the bridge in the first attempt because
              of errors in prediction. However, it quickly realises the need of the bridge in the second attempt. Further,
              it chooses appropriate actions to land the ball in the goal in just the fourth attempt. Note that the robot
              learns to use the bridge effectively; a physical reasoning task reported earlier <a
                href="https://arxiv.org/abs/1907.09620" class="external-link is-normal">[Allen et al., 2020]</a> to be
              challenging to learn for model-free methods, highlighting PhyPlan’s adaptability to long-horizon tasks.
            </p> -->
          <!-- </div> -->
          <!-- OUR DEMO END-->
        </div> 
      </div>
    </div>
    
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Conclusion</h2>
      <div class="column has-text-justified">
        <p>
          We present G<sup>2</sup>TR, a novel approach to
        grounded temporal reasoning. We factorize the problem into
        three major components: (i) candidate interval localization
        in a video based on the required interaction, (ii) fine-grained
        spatial reasoning within the localized interval to ground the
        target object, and (iii) tracking the object post-interaction.
        By leveraging pre-trained visual language models and large
        language models, our approach is zero-shot generalizable
        for both the object set and interactions. 
        </p>
      </div>
      <hr>

      
    </div>
  </section>

  <hr>

  <!-- <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [Allen et al., 2020] Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum.
        Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning.
        Proceedings of the National Academy of Sciences, 117(47):29302–29310, 2020.</code></pre>
      <pre><code>2. [Bakhtin et al., 2019] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick.
        Phyre: A new benchmark for physical reasoning.
        Advances in Neural Information Processing Systems, 32,484 2019.</code></pre>
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{phyplan2024,
      title     = {PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators},
      author    = {Vagadia, Harshil and Chopra, Mudit and Barnawal, Abhinav and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
