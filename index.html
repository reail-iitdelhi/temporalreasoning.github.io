<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="G2TR - Generalized Grounded Temporal Reasoning for Robot
    Instruction Following by Combining Large Pre-trained Models">
  <meta name="keywords" content="Temporal Reasoning, Grounding, Generalized, Robot Instruction Following, Pre-trained Models ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generalized Grounded Temporal Reasoning</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dnerf"></span>G<sup>2</sup>TR - Generalized Grounded Temporal Reasoning for Robot
                Instruction Following via Coupled Pre-trained Models
              </h1>
              <h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);"></h3>
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Riya Arora<sup>1*</sup>,</span>
                <span class="author-block">
                  Niveditha Narendranath<sup>1*</sup>,</span>
                  
                <span class="author-block">
                  Aman Tambi<sup>1+</sup>,</span>
                  <span class="author-block">
                    Sandeep S. Zachariah<sup>1+</sup>,</span>
                
                <br>
                <span class="author-block">
                  Souvik Chakraborty<sup>1</sup>,</span>
                <span class="author-block">
                  Rohan Paul<sup>1</sup></span>
                
                
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <!-- <span class="author-block"><sup>1</sup>Work primarily done when at IIT Delhi, </span>-->
                <span class="author-block"><sup>1</sup>Affiliated with IIT Delhi, </span>
                <span class="author-block"><sup>{*,+}</sup>Indicate equal contributions</span>
              </div>

              <!-- <div class="column has-text-centered">
                <div class="publication-links"> -->
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="http://arxiv.org/abs/2402.15767" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmgQnyyezjnUCMDo3N?e=54ufNt"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/phyplan/PhyPlan"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmcmRXYJJINf42I20?e=XhDGne"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                <!-- </div>
              </div> -->
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  <hr>
<!-- 
  <div class="container is-max-desktop has-text-centered">
    <h2 class="subtitle" , style="max-width: 90%; padding-left: 10%;">
      [A short abstract of the paper goes here.]
    </h2>
  </div> -->

  <section class="section", style="text-align: center;">
    <div class="container is-max-desktop">
      <!-- Introduction. -->
      <h2 class="title has-text-centered is-3">Introduction</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          <!-- Robust Robot Instruction Following mandates robots to possess  spatial and temporal reasoning capabilities within their environment. While current methodologies have advanced robots' semantic and spatial comprehension, temporal reasoning remains deficient. In tasks requiring action execution, robots are expected to draw upon past observations to aid humans effectively, necessitating a human-level semantic understanding and memory. This paper highlights the challenge and proposes solutions essential for enhancing robots' ability to recall sequences of actions, crucial for assisting humans in diverse scenarios. Figure 1 exemplifies the expected memory-dependent action sequences.        </p> -->
          Consider the scenario where a human cleans a
          table and a robot observing the scene is instructed with the task
          <i>“Robot, remove the cloth using which I wiped the table”</i>.Instruction
          following with temporal reasoning requires the robot to identify
          the <b>relevant</b> past object interaction, <b>ground</b> the object of interest
          in the present scene, and <b>execute</b> the task according to the
          human’s instruction. Directly grounding utterances referencing
          past interactions to grounded objects is challenging due to the
          <i>multi-hop</i> nature of references to past interactions and large
          space of object groundings in a video stream observing the
          robot’s workspace. Our key insight is to factor the temporal
          reasoning task as <b>(i)</b> estimating the video interval associated
          with event reference, <b>(ii)</b> performing spatial reasoning over the
          interaction frames to infer the intended object <b>(iii)</b> semantically
          tracking the object’s location till the current scene to enable
          future robot interactions. Our approach leverages existing
          large pre-trained models (which possess inherent generalization
          capabilities) and combines them appropriately for temporal
          grounding tasks. Evaluation on a video-language corpus acquired 
          with a robot manipulator displaying rich temporal
          interactions in spatially-complex scenes displays an average
          accuracy of <b>70.10%</b> indicating the potential of </span>G<sup>2</sup>TR in 
          robot instruction following.
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Technical Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;>
          <!-- Image Column -->
          <div>
            <img src="./static/images/pipeline_diagram_v2.drawio.png"
              style="border-radius: 30px; border: 2px solid #555555; background-color: azure;" />
          </div>
          <!-- Text Column -->
          </div>
        </div>
        <!-- Subtitle Section -->
        <!-- <div class="has-text-justified">
          <h2 class="subtitle">
            <span class="dnerf">PhyPlan</span> is a novel physics-informed planning framework based on accelerated
            learning of Physical Reasoning Tasks using Physics Informed Skill Networks
          </h2>
        </div> -->
      

      <div class="columns is-centered is-vcentered has-text-justified">
        <p>
          <b>Temporal Reasoning Pipeline:</b><br>
          Our method factorize the
          grounded temporal reasoning task as (i) interpreting the
          language instruction as two distinct components: the past
          context and the required robot action, (ii) reasoning of a
          video (or image sequence) over time to localize candidate interval 
          with requisite interaction (iii) performing fine-grained
          spatial reasoning on this interval to ground target object (iv)
          re-acquiring this object post interaction.

          <br><br>
          (i) <b>Temporal Parser</b>: The parser formlulates the two key questions:
          a temporal question that determines "when" the described interaction occurs, 
          and an object-identification question that specifiies "what" objects 
          is involved in the interaction. Additionally, it also
          extracts the action that the robot needs to perform on
          the target object (as shown in figure above!). This is accomplished
          through <i>in-context learning</i> by providing a large language
          model (LLM) with input instruction. 
          <br>
          (ii) <b>Event Localization</b>:
          
          This module performs temporal reasoning on accrued past
          observations to identify the likely interval of the required
          interaction.It takes in the entire video (with requisite object-interaction) 
          and the temporal question from the previous
          module, and returns the specific time instant 
          at which the specified interaction occured. This is implemented 
          via a video-understanding vision language model.
          <br> 
          (iii) <b>Grounding Object of Interest</b>: The purpose of this module is to 
          perform fine-grained
          spatial reasoning and ground the target-object involved in
          the interaction. This module
          operates in three steps: (a) The input is sent to a Target
          Identifier (essentially an image-understanding vision language model) 
          that extracts the class
          of the intended object that needs to be grounded. (b) A class-
          based detection returns the bounding box coordinates of all
          objects belonging to that class, thus providing visual options.
          (c) The Target Identifier (VLM) then identifies the target object 
          by picking the intended target object from these visual options.
          <br>
          (iv) <b>Grounding Propagation via Semantic Tracking</b>: 
          This additional module reasons over the grounded object
          state and location from the time of past interaction till the
          current world state for future robot manipulation. Again, 
          a video-understanding vision language model, with tracking 
          abilities is leveraged for this purpose
        </p>

      </div>
      <!-- <div class="columns is-centered is-vcentered", padding-left: 7%;>
        <img src="./static/images/TR_Pipeline.drawio(3).png"
          style="border-radius: 30px; border: 2px solid #555555; background-color: azure;" />
      </div> -->

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Evaluation Corpus</h2>
      <div class="has-text-justified">
        <p>
          To assess the robot’s ability
          to perform grounded temporal reasoning, we form an
          evaluation corpus of <b>155 video-instruction pairs</b>. The data
          set was collected in a table-top setting with a Franka Emika
          Robot Manipulator observing the scene via a eye-in-hand
          RGB-D camera. A total of 15 objects representating
          common household and healthcare items such as cups,
          bottles, medicines, fruits, notebooks, markers, handkerchiefs
          etc. were used. A total of 8 participants performed
          interactions such as pouring, placing, picking, stacking,
          replacing, wiping, dropping, repositioning, swapping etc.
          with 3 − 6 objects in each scene. The human participants
          also provided natural language instructions for the robot
          referencing to interactions and objects in the preceding
          interactions in the workspace. This resulted in a corpus
          of 155 video instruction pairs (each between 6 − 30
          seconds). The instructions and interactions expressed natural
          diversity of spatial and temporal reasoning complexity.
          For detailed analysis, the evaluation corpus is bifurcated
          as: 
          <b>(i) single/multi hop</b> temporal reasoning (56 : 99)
          <b>(ii) simple/complex spatial</b> grounding (98 : 57), <b>(iii)
            single/multiple interactions</b> (62 : 93) and <b>(iv) partial or full
              observation</b> of the referred object (36 : 119)
        

        </p>
        <br>
      </div>
      <hr>
      <div class="columns is-centered is-vcentered" , padding-left: 7%;>
        <!-- Image Column -->
        <!--div>
          <img src="./static/images/dataset_classification.drawio.png"
            style="border-radius: 30px; border: 2px solid #555555; background-color: azure; scale: 80%;" />
        </div-->
        <table>
          <tr>
              <th style="text-align: center; font-size: 20px;" colspan="2"><u>Reasoning Complexity</u></th>
              <th style="text-align: center; font-size: 20px;" colspan="2"><u>Interaction Complexity</u></th>
          </tr>
          <tr>
              <td 
              style="text-align: center;">
                  <strong>Single Hop</strong><br>
                  <i><b>Instruction: </b>Robot, pick the cloth that I just dropped!</i><br>
                  <img src="static/gif/EvalCorpus/single_hop.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Multi Hop</strong><br>
                  <i><b>Instruction: </b>Robot, give me <br>the object placed second!</i><br>
                  <img src="static/gif/EvalCorpus/multi_hop.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Single Interaction</strong><br>
                  <i><b>Instruction: </b>Robot, give the cloth that was just placed!</i><br>
                  <img src="static/gif/EvalCorpus/single_interaction.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Multi Interaction</strong><br>
                  <i><b>Instruction: </b>Robot, pick cup in which water was poured by orange bottle!</i><br>
                  <img src="static/gif/EvalCorpus/multi_interaction.gif" style="width: 200px; height: 150px;">
              </td>
          </tr>
      
          <tr>
              <th style="text-align: center; font-size: 20px;"  colspan="2"><u>Spatial/Visual Complexity</u></th>
              <th style="text-align: center; font-size: 20px;" colspan="2"><u>Observability</u></th>
          </tr>
          
          <tr>
              <td style="text-align: center;">
                  <strong>Spatially Simple</strong><br>
                  <i><b>Instruction: </b>Robot, remove the cloth used for wiping!</i><br>
                  <img src="static/gif/EvalCorpus/spatially_simple.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Spatially Complex</strong><br>
                  <i><b>Instruction: </b>Robot, point to the cup which was just placed on the table</i><br>
                  <img src="static/gif/EvalCorpus/spatially_complex.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Completely Observable</strong><br>
                  <i><b>Instruction: </b>Robot, pick the object that was just placed!</i><br>
                  <img src="static/gif/EvalCorpus/completely_obs.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Partially Observable</strong><br>
                  <i><b>Instruction: </b>Robot, where <br>is the medicine?</i><br>
                  <img src="static/gif/EvalCorpus/partially_obs.gif" style="width: 200px; height: 150px;">
              </td>
          </tr>
        </table>
        <!-- Text Column -->
        </div>
      </div>
      <!-- <div class="columns is-multiline" style="justify-content: center;">
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1i1o-place_cloth.gif" alt="GIF 1" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Single Interaction - Similar Objects</h3>

        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1imo-place.gif" alt="GIF 2"
              style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Multiple Interactions - Distinct Objects</h3>

        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1a2i1o-pour_cup.gif" alt="GIF 3" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">One Action - Multiple Interactions - Similar Objects</h3>

        </div>
        
      </div>

      <div class="columns is-multiline" style="justify-content: center;">
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/1imo.gif" alt="GIF 4" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Single Action - Single Interaction - Distinct Objects</h3>

        </div>
        <div class="column is-one-third" style="display: flex; flex-direction: column; align-items: center;">
          <figure class="image" style="display: flex; justify-content: center; align-items: center; margin: auto;">
            <img src="static/gif/GIFs/tmp_reason24.gif" alt="GIF 5" style="width: 300px; height: 200px; border-radius: 25px;">
          </figure>
          <h3 class="title is-5" style="text-align: center;">Multiple Action - Multiple Interaction - Distinct Objects</h3>

        </div>
      </div> -->

      <!-- <div class="columns is-centered is-vcentered"> -->
        <!-- Text Column -->
        <!-- <div class="column is-half has-text-justified">
          <p>
            <i><b>The skill learning model is based on a neural network that predicts the object's state during dynamic
                interaction continuously parameterised by time.</i></b> The figure on the side shows the predicted
            positions of the ball plotted against time in the Bounce Task. Such interactions can be simulated in a
            physics engine by using
            numerical integration schemes.
            However, since we aim to perform multi-step interactions, simulating outcomes during training is often
            intractable. Hence, we adopt a learning-based approach and learn a function which predicts the object's
            state during dynamic interaction continuously parameterised by time.
            For certain skills like swining, sliding and throwing we levarage the known governing physics equations and
            employ a physics-informed loss function in neural network to constrain the latent space, these are called as
            Physics-Informed Skill Networks.
            However, skills like bouncing and hitting are learnt directly from data because of complex and intractable
            physics.
          </p>
        </div> -->
        <!-- Image Column -->
        <!-- <div class="column is-half">
          <img src="./static/images/bounce_chain_0.5.png" width="1499" height="976"
            style="border-radius: 25px; border: 2px solid #000000;" />
        </div> -->
      <!-- </div> -->
      <!-- <div class="has-text-justified">
        <p>
          The skill learning model is based on a neural network that predicts the object's state during dynamic
          interaction continuously parameterised by time. Such interactions can be simulated in a physics engine by
          using numerical integration schemes.
          However, since we aim to perform multi-step interactions, simulating outcomes during training is often
          intractable. Hence, we adopt a learning-based approach and learn a function which predicts the object's
          state during dynamic interaction continuously parameterised by time.
          For certain skills like swining, sliding and throwing we levarage the known governing physics equations and
          employ a physics-informed loss function in neural network to constrain the latent space, these are called as
          Physics-Informed Skill Networks.
          However, skills like bouncing and hitting are learnt directly from data because of complex and intractable
          physics.
        </p>
      </div> -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;"> Video Demonstration</h2>
      <div class="columns is-centered is-vcentered"> 
        <Text Column>
        <div class="column has-text-justified">
          <p>
            <i><b>Demonstration 1: Video showing the robot executing the task for the instruction "pick the object that I just placed."</i></b> <br>
          
            The robot in scene is a Franka Emika Arm Manipulator. There is a table in front of the robot, on which various objects such as a pair of glasses, a cup, 
            and a small bottle are already kept and the robot is observing this scene with its camera. Then a person comes and places a new object (a banana) on the table. 
            Then an instruction "pick the object that I just placed" is given to the robot. 
          </p>
        </div> 
        <Image Column>
        <div class="column is-centered">
          <video id="matting-video" controls playsinline height="70%" width="70%" style="border-radius: 25px;">
            <source src="./static/video/video_demo.mp4" type="video/mp4">
          </video> 
          <div class="column has-text-justified">
            <p>
              <i><b>Demonstration 2: Video showing the robot responding to the question "where is the banana?"</i></b> <br>
              The robot in scene is a Franka Emika Arm Manipulator. There is a table in front of the robot, on which two books,two markers and a bag is placed.
              The robot is observing this scene with its camera. Then the robot is given the instruction "where is the marker that I just used". 
              The robot calls the G<sup>2</sup>TR pipeline to identify the object being referred and points
              to the bag inside which the marker was placed.
            </p>
          </div> 
          <Image Column>
          <div class="column is-centered">
            <video id="matting-video" controls playsinline height="70%" width="70%" style="border-radius: 25px;">
              <source src="./static/video/marker_pointing.mp4" type="video/mp4">
            </video> 
          </div>
          

          <!-- Row 2: Text and Video for Demo 2 -->
          <div class="column is-half">
              <p>
                  <i><b><u>Demonstration 4: Multi-hop reasoning scenario</u>: Video showing the robot executing the instruction "Give me the bottle placed second by the human"</b></i> <br>
                  The robot, equipped with a camera, observes a shelf as a human sequentially places three bottles on it.
                  Later, another individual instructs the robot to retrieve the second bottle placed. To execute this task, 
                  the robot leverages the G<sup>2</sup>TR pipeline, 
                  which performs multi-hop reasoning to identify the correct bottle and the robot successfully fulfills the command.
              </p>
              <br>
              <video id="matting-video" controls playsinline height="100%" width="100%" style="border-radius: 25px;">
                <source src="./static/video/DemoVideo3.mp4" type="video/mp4">
            </video>
          </div>
        </div> 
      </div> 
    </div>  
  </section>>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experimental Insights</h2>
      <div id="tasks" , class="column has-text-justified">
        <p>
          We have compared the performance of the proposed
          model G<sup>2</sup>TR with alternative approaches -Direct Temporal Visual Grounding (DTVG) and Refined Temporal Visual Grounding (RTVG).
           The proposed approach has better performance 
          than suggested alternate approaches by 26%, in terms
          of overall average grounding accuracy.
          We compared the state of the art LVLM and LLM models to realize various aspects of the proposed pipeline. We use GPT4 for both the parsing tasks, GPT-4V for initial scene description, Video-LLaVA for object interaction recognition and CogVLM for semantic object grounding. The experimentation of the entire end-to-end pipeline was carried out on a Franka Emika robot manipulator. The robot observes a person placing an object and later successfully executes the instruction, "robot, pick up the object that I just placed". 
          The figure below show the accuracy of instruction following for the scenarios partitioned as those containing identical vs. distinct objects on the scene. The overall success rate was obtained as 45.3%. We observe that LVLMs gave incorrect or ambiguous outputs when all the objects on the scene were identical while higher performance for scenarios with distinct objects. Results indicate the lack of robustness in contemporary LVLMs in identifying complex spatial properties.  
        </p>
      </div>
      <hr>
      <div class="columns is-multiline">
        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Component-Wise Analysis of Failures</h3>
            <img src="static/images/component_analysis_v4.drawio.png" alt="GIF 4" style="width: 600px; height: 280px; border-radius: 25px;">

            <p class="teaser has-text-justified">
              Component-Wise Analysis frequency of failure cases generated by each module</p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Failure Case Examples</h3>
            
            
              <!-- First Row of Images -->
              <div class="columns">
                <div class="column is-centered is-one-third">
                    <figure style="position: relative;">
                      <figcaption style="text-align: center; margin-bottom: 10px; font-size: 15px;"><i><b>Instruction: </b>Robot, pick bottle placed just before orange bottle</i></figcaption>
                      <img src="static/gif/EvalCorpus/failure_case.gif" alt="Image 1 Description" style="width:100%;">
                      <p style="text-align: center;"><b>(a) Incorrect time instant</b> (Expected: 5, G<sup>2</sup>TR: 12)</p>
                    </figure>
                </div>
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px; font-size: 15px;"><i><b>Instruction: </b>Pick the container in which water was poured</i></figcaption>
                      <img src="static/gif/EvalCorpus/failure_case2.gif" alt="Image 2 Description" style="width:100%;">
                      <p style="text-align: center;"><b>(b) Incorrect Object Class</b> (Expected: Bottle, G<sup>2</sup>TR: Cup)</p>
                    </figure>
                </div>
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px;font-size: 15px;"><i><b>Instruction: </b>Pick the container by which water was poured</i></figcaption>
                      <img src="static/images/failure_case_4.png" alt="Image 3 Description" style="width:100%; height:110 px">
                      <p style="text-align: center;"><b>(c) Visual Options Ambiguity</b> (Object labels overlapping)</p>
                    </figure>
                </div>
            </div>

            <!-- Second Row of Images -->
            <div class="columns">
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px;font-size: 15px;"><i><b>Instruction: </b>Robot, pick the cup under<br> which strawberry is hidden!</i></figcaption>
                      <img src="static/gif/EvalCorpus/tracking_lost.gif" alt="Image 4 Description" style="width:60%;">
                      <p style="text-align: center;"><b>(d) Incorrect Object Tracking</b><br> (Wrong cup tracked)</p>
                    </figure>
                </div>
                <!-- <div class="column">
                    <figure>
                      <img src="static/images/failure_case_3.png" alt="Image 5 Description" style="width:100%;">
                    </figure>
                </div> -->
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px;font-size: 15px;"><i><b>Instruction: </b>Robot, point to the cup which<br> was just used by the person</i></figcaption>
                      <img src="static/images/failure_case_5.png" alt="Image 6 Description" style="width:60%;">
                      <p style="text-align: center;"><b>(e) Insufficient Grounding</b> <br>(One cup not grounded)</p>
                    </figure>
                </div>
            </div>
  

                
              
              
            <!-- <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;
              <source src="./static/video/slide.mp4" type="video/mp4">
            </video> -->
            <div class="is-flex is-justify-content-center">
              <p class="teaser has-text-justified" style="max-width: 600px;">
                Examples of Failure Cases displays instances of failure within G<sup>2</sup>TR framework.
              </p>
            </div>
          </div>
        </div>

        <!-- <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Example - Failure Case 1</h3>
            <p class="has-text-justified">Scenario in which all objects are completely identical and the spatial properties get complex (objects are too close to each-other)</p>
            
              <img src="static/images/Untitled Diagram.drawio.png" alt="GIF 4" style="width: 500px; height: 315px; border-radius: 25px;">
            
            <p class="teaser has-text-justified">
              For instruction ”pick the object that was just placed”, VideoLLaVA outputs "The object that was placed was the cup in the center". Hence, the output of our proposed pipeline (via Video-LLAVA) (in red) deviates from the ground truth (in green) resulting in erronous temporal reasoning.
            </p>
          </div>
        </div> 

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Example - Failure Case 2</h3>
            <p class="has-text-justified"> Scenario in which two identical objects are placed among other distinct objects.</p>
            
              <img src="static/images/amb2.drawio.png" alt="GIF 4" style="width: 500px; height: 315px; border-radius: 25px;">
            </video>
            <p class="teaser has-text-justified">
              For instruction ”pick the object that was just placed”, VideoLLaVA outputs "The object that was placed was the controller near the person." Hence, the output of our proposed pipeline (via Video-LLAVA) (in red) deviates from the ground truth (in green) resulting in erronous temporal reasoning.
              <These, along with multiple physical skills involved in the task, highlight PhyPlan's adaptability to long-horizon tasks. -->
            <!-- </p>
          </div>  -->
          <!-- OUR DEMO START -->
          <!-- <div class="content">
            <h3 class="title is-4 has-text-centered"><span class="dnerf">DQN-Adaptive</span></h3>
            <div class="columns is-centered is-vcentered">
              <!-- Image Column -->
              <!-- <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-1.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 1
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-4.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 4
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/DQN-11.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">DQN-Adaptive</span>: Action 11
                </p>
              </div>
            </div>
            <!-- Subtitle Section -->
            <!-- <p class="teaser has-text-justified">
              <span class="dnerf">DQN-Adaptive</span> (Baseline) executes actions in sequence while learning the difference
              in the predicted reward for an action and the actual reward. However, it does not use the bridge even after 11
              attempts which is needed to land the ball further closer to the goal.
            </p>
          </div> --> 
          <!-- <div class="content">
            <h3 class="title is-4 has-text-centered"><span class="dnerf">PhyPlan</span></h3>
            <div class="columns is-centered is-vcentered">
              <!-- Image Column -->
              <!-- <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_1.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 1
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_2.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 2
                </p>
              </div>
              <div class="column is-one-third">
                <img src="./static/images/bridge_dqn-comp/Phyplan_4.png" width="400" height="300"
                  style="border-radius: 25px;" />
                <p class="teaser has-text-centered">
                  <span class="dnerf">PhyPlan</span>: Action 4
                </p>
              </div>
            </div> --> 
            <!-- Subtitle Section -->
            <!-- <p class="teaser has-text-justified">
              <span class="dnerf">PhyPlan</span> executes actions in sequence while learning the difference in the predicted
              reward for an action and the actual reward. It does not use the bridge in the first attempt because
              of errors in prediction. However, it quickly realises the need of the bridge in the second attempt. Further,
              it chooses appropriate actions to land the ball in the goal in just the fourth attempt. Note that the robot
              learns to use the bridge effectively; a physical reasoning task reported earlier <a
                href="https://arxiv.org/abs/1907.09620" class="external-link is-normal">[Allen et al., 2020]</a> to be
              challenging to learn for model-free methods, highlighting PhyPlan’s adaptability to long-horizon tasks.
            </p> -->
          <!-- </div> -->
          <!-- OUR DEMO END -->
        </div> 
      </div>
    </div>
    
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Conclusion</h2>
      <div class="column has-text-justified">
        <p>
        We present G<sup>2</sup>TR, a novel approach to
        grounded temporal reasoning. We factorize the problem into
        three key components: (i) candidate interval localization
        in a video based on required interactions, (ii) fine-grained
        spatial reasoning within the localized interval to ground the
        target object, and (iii) tracking the object post-interaction.
        By leveraging pre-trained visual language models and large
        language models, G<sup>2</sup>TR achieves zero-shot generalization for
        both object set and interactions. We also propose a dataset
        of 155 video-instruction pairs covering spatially complex,
        multi-hop, partially observable, and multi-interaction tempo-
        ral reasoning tasks. Evaluation on the dataset shows signif-
        icant improvement over alternative approaches, highlighting
        G<sup>2</sup>TR’s potential in robot instruction following. Finally, it
        is important to note that G2TR currently has two limits: (i)
        it can only process videos up to one minute, as constrained
        by the video-reasoning model, and (ii) it can ground only
        a single object at a time. We aim to overcome both these
        limitations in future.
        </p>
      </div>
      <hr>

      
    </div>
  </section>

  <hr>

  <!-- <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [Allen et al., 2020] Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum.
        Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning.
        Proceedings of the National Academy of Sciences, 117(47):29302–29310, 2020.</code></pre>
      <pre><code>2. [Bakhtin et al., 2019] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick.
        Phyre: A new benchmark for physical reasoning.
        Advances in Neural Information Processing Systems, 32,484 2019.</code></pre>
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{phyplan2024,
      title     = {PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators},
      author    = {Vagadia, Harshil and Chopra, Mudit and Barnawal, Abhinav and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
