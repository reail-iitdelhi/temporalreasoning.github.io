<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="G2TR - Generalized Grounded Temporal Reasoning for Robot
    Instruction Following by Combining Large Pre-trained Models">
  <meta name="keywords" content="Temporal Reasoning, Grounding, Generalized, Robot Instruction Following, Pre-trained Models ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Generalized Grounded Temporal Reasoning</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dnerf"></span>G<sup>2</sup>TR - Generalized Grounded Temporal Reasoning for Robot
                Instruction Following via Coupled Pre-trained Models
              </h1>
              <h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);"></h3>
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Riya Arora<sup>1*</sup>,</span>
                <span class="author-block">
                  Niveditha Narendranath<sup>1*</sup>,</span>
                  
                <span class="author-block">
                  Aman Tambi<sup>1+</sup>,</span>
                  <span class="author-block">
                    Sandeep S. Zachariah<sup>1+</sup>,</span>
                
                <br>
                <span class="author-block">
                  Souvik Chakraborty<sup>1</sup>,</span>
                <span class="author-block">
                  Rohan Paul<sup>1</sup></span>
                
                
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <!-- <span class="author-block"><sup>1</sup>Work primarily done when at IIT Delhi, </span>-->
                <span class="author-block"><sup>1</sup>Affiliated with IIT Delhi, </span>
                <span class="author-block"><sup>{*,+}</sup>Indicate equal contributions</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links"> 
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>-->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2410.07494" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> 
                  <!-- Video Link. -->
                  <span class="link-block">
                    <a href="https://csciitd-my.sharepoint.com/:f:/g/personal/souvik_iitd_ac_in/ErrN2ZynFm5JrzVeGpwm1NQB8s1k42s4UMwDCx-9Rv23jg?e=ocHyrL"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> 
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/Project-LPEA/embodied_temporal_reasoning"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> 
                  <!-- Dataset Link. -->
                  <span class="link-block">
                    <a href="https://csciitd-my.sharepoint.com/:f:/g/personal/souvik_iitd_ac_in/EpUatwuzM6FJur79k1FIKKUB0A9U1zDwpEO-qaP58QPrag?e=wc6Wts"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> 
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  <hr>


  <section class="section", style="text-align: center;">
    <div class="container is-max-desktop">
      <!-- Introduction. -->
      <h2 class="title has-text-centered is-3">Introduction</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          
          Consider the scenario where a human cleans a
          table and a robot observing the scene is instructed with the task
          <i>“Robot, remove the cloth using which I wiped the table”</i>. Instruction
          following with temporal reasoning requires the robot to identify
          the <b>relevant</b> past object interaction, <b>ground</b> the object of interest
          in the present scene, and <b>execute</b> the task according to the
          human’s instruction. Directly grounding utterances referencing
          past interactions to grounded objects is challenging due to the
          <i>multi-hop</i> nature of references to past interactions and large
          space of object groundings in a video stream observing the
          robot’s workspace. Our key insight is to factor the temporal
          reasoning task as <b>(i)</b> estimating the video interval associated
          with event reference, <b>(ii)</b> performing spatial reasoning over the
          interaction frames to infer the intended object <b>(iii)</b> semantically
          tracking the object’s location till the current scene to enable
          future robot interactions. Our approach leverages existing
          large pre-trained models (which possess inherent generalization
          capabilities) and combines them appropriately for temporal
          grounding tasks. Evaluation on a video-language corpus acquired 
          with a robot manipulator displaying rich temporal
          interactions in spatially-complex scenes displays an average
          accuracy of <b>70.10%</b> indicating the potential of </span>G<sup>2</sup>TR in 
          robot instruction following.</p>
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Technical Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;>
          <!-- Image Column -->
          <div>
            <img src="./static/gif/EvalCorpus/tech_overview.gif"
              style="width:1000px ; border-radius: 30px; border: 2px solid #555555; background-color: azure;" />
          </div>
          <!-- Text Column -->
          </div>
        </div>
        
      

      <div class="columns is-centered is-vcentered has-text-justified">
        <p>
          <b>Generalized Grounded Temporal Reasoning (</span>G<sup>2</sup>TR) Pipeline:</b><br>
          We address the temporal reasoning task by grounding
          the requisite interaction in the input-video for the candidate
          interval extraction, and then pinpointing the intended object
          through fine-grained spatial reasoning within this interval for
          grounding. Our framework comprises three components:
          (1) Temporal-Parser (TP) (2) Candidate interval estimation
          via Event-Localizer (EL), and (3) Grounding object of inter-
          est via Target-Detector (TD) followed by object re-acquisition 
          via semantic tracking

          <br><br>
          (i) <b>Temporal Parser</b>: The parser formlulates the two key questions:
          a temporal question that determines "when" the described interaction occurs, 
          and an object-identification question that specifiies "what" objects 
          is involved in the interaction. Additionally, it also
          extracts the action that the robot needs to perform on
          the target object (as shown in figure above!). This is accomplished
          through <i>in-context learning</i> by providing a large language
          model (LLM) with input instruction. 
          <br>
          (ii) <b>Event Localization</b>:
          
          This module performs temporal reasoning on accrued past
          observations to identify the likely interval of the required
          interaction.It takes in the entire video (with requisite object-interaction) 
          and the temporal question from the previous
          module, and returns the specific time instant 
          at which the specified interaction occured. This is implemented 
          via a video-understanding vision language model.
          <br> 
          (iii) <b>Grounding Object of Interest</b>: The purpose of this module is to 
          perform fine-grained
          spatial reasoning and ground the target-object involved in
          the interaction. This module
          operates in three steps: (a) The input is sent to a Target
          Identifier (essentially an image-understanding vision language model) 
          that extracts the class
          of the intended object that needs to be grounded. (b) A class-
          based detection returns the bounding box coordinates of all
          objects belonging to that class, thus providing visual options.
          (c) The Target Identifier (VLM) then identifies the target object 
          by picking the intended target object from these visual options.
          <br>
          (iv) <b>Grounding Propagation via Semantic Tracking</b>: 
          This additional module reasons over the grounded object
          state and location from the time of past interaction till the
          current world state for future robot manipulation. Again, 
          a video-understanding vision language model, with tracking 
          abilities is leveraged for this purpose
        </p>

      </div>
      

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Evaluation Corpus</h2>
      <div class="has-text-justified">
        <p>
          To assess the robot’s ability
          to perform grounded temporal reasoning, we form an
          evaluation corpus of <b>155 video-instruction pairs</b>. The data
          set was collected in a table-top setting with a Franka Emika
          Robot Manipulator observing the scene via a eye-in-hand
          RGB-D camera. A total of 15 objects representating
          common household and healthcare items such as cups,
          bottles, medicines, fruits, notebooks, markers, handkerchiefs
          etc. were used. A total of 8 participants performed
          interactions such as pouring, placing, picking, stacking,
          replacing, wiping, dropping, repositioning, swapping etc.
          with 3 − 6 objects in each scene. The human participants
          also provided natural language instructions for the robot
          referencing to interactions and objects in the preceding
          interactions in the workspace. This resulted in a corpus
          of 155 video instruction pairs (each between 6 − 30
          seconds). The instructions and interactions expressed natural
          diversity of spatial and temporal reasoning complexity.
          For detailed analysis, the evaluation corpus is bifurcated
          as: 
          <b>(i) single/multi hop</b> temporal reasoning (56 : 99)
          <b>(ii) simple/complex spatial</b> grounding (98 : 57), <b>(iii)
            single/multiple interactions</b> (62 : 93) and <b>(iv) partial or full
              observation</b> of the referred object (36 : 119)
        

        </p>
        <br>
      </div>
      <hr>
      <div class="columns is-centered is-vcentered" , padding-left: 7%;>
       
        <table>
          <tr>
              <th style="text-align: center; font-size: 20px;" colspan="2"><u>Reasoning Complexity</u></th>
              <th style="text-align: center; font-size: 20px;" colspan="2"><u>Interaction Complexity</u></th>
          </tr>
          <tr>
              <td 
              style="text-align: center;">
                  <strong>Single Hop</strong><br>
                  <i><b>Instruction: </b>Robot, pick the cloth that I just dropped!</i><br>
                  <img src="static/gif/EvalCorpus/single_hop.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Multi Hop</strong><br>
                  <i><b>Instruction: </b>Robot, give me <br>the object placed second!</i><br>
                  <img src="static/gif/EvalCorpus/multi_hop.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Single Interaction</strong><br>
                  <i><b>Instruction: </b>Robot, give the cloth that was just placed!</i><br>
                  <img src="static/gif/EvalCorpus/single_interaction.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Multi Interaction</strong><br>
                  <i><b>Instruction: </b>Robot, pick cup in which water was poured by orange bottle!</i><br>
                  <img src="static/gif/EvalCorpus/multi_interaction.gif" style="width: 200px; height: 150px;">
              </td>
          </tr>
      
          <tr>
              <th style="text-align: center; font-size: 20px;"  colspan="2"><u>Spatial/Visual Complexity</u></th>
              <th style="text-align: center; font-size: 20px;" colspan="2"><u>Observability</u></th>
          </tr>
          
          <tr>
              <td style="text-align: center;">
                  <strong>Spatially Simple</strong><br>
                  <i><b>Instruction: </b>Robot, remove the cloth used for wiping!</i><br>
                  <img src="static/gif/EvalCorpus/spatially_simple.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Spatially Complex</strong><br>
                  <i><b>Instruction: </b>Robot, point to the cup which was just placed on the table</i><br>
                  <img src="static/gif/EvalCorpus/spatially_complex.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Completely Observable</strong><br>
                  <i><b>Instruction: </b>Robot, pick the object that was just placed!</i><br>
                  <img src="static/gif/EvalCorpus/completely_obs.gif" style="width: 200px; height: 150px;">
              </td>
              <td style="text-align: center;">
                  <strong>Partially Observable</strong><br>
                  <i><b>Instruction: </b>Robot, where <br>is the medicine?</i><br>
                  <img src="static/gif/EvalCorpus/partially_obs.gif" style="width: 200px; height: 150px;">
              </td>
          </tr>
        </table>

        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3" style="text-align: center;">Video Demonstrations</h2>
        <div class="columns is-multiline">
            
            <!-- Row 1: Text and Video for Demo 1 -->
            <div class="column is-half">
                <p>
                    <i><b><u>Demonstration 1 - A Simple Scenario</u>: Video showing the robot executing the instruction "Pick the object that I just placed."</b></i> <br>
                    The robot in scene is a Franka Emika Arm Manipulator. There is a table in front of the robot, on which various objects (glasses, a cup, 
                    a small bottle) are already kept and the robot is observing this scene with its camera. Then a person places a banana on the  table and an instruction to pick the object that was just placed is given, which the robot executes.

                    
                </p>
                <br>
                <video id="matting-video" controls playsinline height="100%" width="100%" style="border-radius: 25px;">
                  <source src="./static/video/DemoVideo1.mp4" type="video/mp4">
              </video> 
            </div>
            

            <!-- Row 2: Text and Video for Demo 2 -->
            <div class="column is-half">
                <p>
                    <i><b><u>Demonstration 2- Visually complex scenario with multiple interactions</u>: Video showing the robot carrying out instruction "Robot, remove the cloth that was used to wipe the table"</b></i> <br>
                    The robot observes a table with four cloths (two green, one blue, one pink) and a cleaning liquid. 
                    After a human rearranges the items and cleans with one cloth, the robot is instructed to remove the used cloth. 
                    It then uses the G<sup>2</sup>TR pipeline to identify and dispose of the correct cloth in the bin.
                </p>
                <br>
                <video id="matting-video" controls playsinline height="100%" width="100%" style="border-radius: 25px;">
                  <source src="./static/video/DemoVideo4.mp4" type="video/mp4">
              </video>
            </div>
            <!-- Row 1: Text and Video for Demo 1 -->
            <div class="column is-half">
              <p>
                  <i><b><u>Demonstration 3 - Scenario involving partial observability</u> : Video showing robot responding to the question "Robot, where is the marker that I just used?"</b></i> <br>
                  The robot, with its camera, observes a table with two books, two markers, and a bag. 
                  A human uses a marker, places it in a book, and then puts the book in the bag. 
                  The robot is instructed to identify and point to the partially occluded marker. 
                  Using the G<sup>2</sup>TR pipeline, it successfully identifies and points to the bag containing the marker.
              </p>
              <br>
              <video id="matting-video" controls playsinline height="100%" width="100%" style="border-radius: 25px;">
                <source src="./static/video/DemoVideo2.mp4" type="video/mp4">
            </video> 
          </div>
          

          <!-- Row 2: Text and Video for Demo 2 -->
          <div class="column is-half">
              <p>
                  <i><b><u>Demonstration 4: Multi-hop reasoning scenario</u>: Video showing the robot executing the instruction "Give me the bottle placed second by the human"</b></i> <br>
                  The robot, equipped with a camera, observes a shelf as a human sequentially places three bottles on it.
                  Later, another individual instructs the robot to retrieve the second bottle placed. To execute this task, 
                  the robot leverages the G<sup>2</sup>TR pipeline, 
                  which performs multi-hop reasoning to identify the correct bottle and the robot successfully fulfills the command.
              </p>
              <br>
              <video id="matting-video" controls playsinline height="100%" width="100%" style="border-radius: 25px;">
                <source src="./static/video/DemoVideo3.mp4" type="video/mp4">
            </video>
          </div>
        </div> 
    </div>
</section>

<section class="section", style="text-align: center;"></section>
    <div class="container is-max-desktop">
      <!-- Prompts. -->
      <h2 class="title has-text-centered is-3"> Prompting Details</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          To achieve generalized grounded temporal reasoning, 
          G<sup>2</sup>TR utilizes the reasoning capabilities of several 
          pre-trained large models. To ensure these models perform 
          effectively and appropriately for the designated tasks, 
          various prompts and prompting strategies were explored, as detailed below.
          <br><br>
          <b>Temporal Parser</b>: For this parser, an LLM 
          <a href="https://arxiv.org/abs/2303.08774" class="external-link is-normal">[GPT-4]</a> 
          
          was employed. 
          This LLM-based parser leverages in-context learning with a few-shot approach. 
          The prompt provides a comprehensive background on the robotic setting, enabling it to generate a temporal
          question, object identification question, and robot actions—that serve as essential inputs for subsequent modules.
          <pre><code>
prompt_temporal_parser: |
"""
          
    example_parsing  = [
          {
              "instruction" : "Point to the bottle that I just placed.",
  
              "ground_truth" : {
                  "temporal_question": "When is placing by the person happening in the video? 
                  Give the exact timestamp.",
                  "action": "Point to",
                  "object_identification_question": "Identify the bottle that is being placed in these frames."
  
              },
          },
          {
              "instruction" : "Remove the object that was first eaten",
              "ground_truth" : {
                  "temporal_question": "When is eating of first object by the person happening in the video? 
                  Give the exact timestamp.",
                  "action": "Remove",    
                  "object_identification_question": "Identify the object that is being by the person in these frames."
              },
          },
          {
              "instruction" : "Remove the object that was placed last",
              "ground_truth" : {
                  "temporal_question": "When is placing of last object by the person happening in the video? 
                  Give the exact timestamp.",
                  "action": "Remove",    
                  "object_identification_question": "Identify the object that is being placed by the person 
                  in these frames."
              },
          },
              {
              "instruction" : "Where is the apple?",
              "ground_truth" : {
                  "temporal_question": "When was the apple last seen? Give the exact timestamp.",
                  "action": "",   
                  "object_identification_question": "Where is the apple?" 
              },
          },
          {
              "instruction" : "Identify the object that was used for cleaning second by the girl",
              "ground_truth" : {
                  "temporal_question": "When is the cleaning using a second object by the girl happening in the video? 
                  Give the exact timestamp.",
                  "action": "Identify",    
                  "object_identification_question": "Identify the object used for cleaning by the girl in these frames."
              },
          },
  
          {
              "instruction" : "Remove the cloth that was used by the boy",
              "ground_truth" : {
                  "temporal_question": "When is the using of cloth by the boy happening in the video? 
                  Give the exact timestamp.",
                  "action": "Remove",    
                  "object_identification_question": "Identify the cloth used by the boy in these frames."
                  "object" : "cloth used by boy",
                  "interaction": human-interaction with cloth by the boy
              },
          },
          {
              "instruction" : "Retrieve the bottle that was filled first by the girl",
              "ground_truth" : {
                  "temporal_question": "When is filling of the first bottle happening in the video? 
                  Give the exact timestamp.",
                  "action": "Retrieve",    
                  "object_identification_question": "Identify the object that is being by the person in these frames."
                  "object" : "bottle that was filled by the girl",
                  "interaction" : "bottle filling by the girl"
              },
          },
        
    ]
  
There is a robot that needs to take a human instruction and formulate a temporal question as per instruction, what action 
to take and the object of interest. Given the human instruction, return a dictionary with 'temporal_question', 'action', 
'object_identification_question', 'object', and 'interaction' as keys. For the 'object' and 'interaction' keys, remove 
the temporal aspect and clues such as 'at 2nd second' or 'last'. Return answer in JSON format always. 
"""
          </code></pre>
          <br>
          <b>Event Localizer (Candidate Interval Extraction): </b>
          This module leverages the temporal reasoning capabilities of a video-understanding VLM
          <a href="https://arxiv.org/abs/2408.16500" class="external-link is-normal">[CogVLM2-video]</a>,
          using the <b>temporal question</b> generated by the aforementioned temporal parser as its input. 
          <pre><code>
event_localizer_prompt = json ( temporal_parser_output ) [" temporal_question "]
          </code></pre>          
          <br>

          <b>Grounding object of interest: </b>
          The workflow of this module consists of three steps: the first and third steps utilize
          spatial reasoning and image understanding capabilities of a Vision-Language Model, 
          <a href="https://arxiv.org/abs/2303.08774" class="external-link is-normal">[GPT-4]</a>
          while the second step employs
          grounding capabilities of a Phrase Grounding Model. 
          <a href="https://arxiv.org/abs/2405.10300" class="external-link is-normal">[Grounding DINO 1.5 Pro]</a>
          .The prompt for this module, again derived from the temporal
          parser output, utilizes the object identification question. In the first step, the module retrieves the <b><i>class</i></b> of the object
          involved in the language-referenced human interaction. In the third step, using the same prompt with an added emphasis,
          it is used to retrieve the <b><i>object label</i></b> from given visual options.
          <br>

          <pre><code>
target_detector_step1_prompt = json(temporal_parser_output)["object_identification_question"] + 
"Return just the object class."
          </code></pre>
          <pre><code>
target_detector_step3_prompt = json(temporal_parser_output)["object_identification_question"] + 
"Return just the object label."
          </code></pre>
          <br>
          <b>Partial Observability Cases:</b>
          In these cases, as the target object becomes partially observable, the prompts are designed
          to retrieve the object that is closely associated with or occluding the target-object.
          <pre><code>
object_class = target_ detector_step1_output

"Where did " + object_class + " go? Give the object that partially or completely hid the " + object_class.
          </code></pre>
          <br>

          Example if the target object is a <b><i>marker</i></b>, which was kept inside a <b><i>book</i></b>, and the book was put in a <b><i>bag</i></b>, the prompts would be:
          <pre><code>
"Where did the <i>marker</i> go? Give the object that partially or completely hid the <i>marker</i>." 

## And once the marker goes out of view and the occluding object is determined to be a book:

"Where did the <i>book</i> go? Give the object that partially or completely hid the <i>book</i>."
          </code></pre>

          Thus, the prompts are iteratively refined and re-issued until the final occluding object is identified.

  

      </div>
  </section>

  <section class="section", style="text-align: center;"></section>
    <div class="container is-max-desktop">
      <!-- Alternate Approaches. -->
      <h2 class="title has-text-centered is-3">Alternate Approaches (Baselines)</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          
        The proposed model, G<sup>2</sup>TR, was compared against two baseline approaches, 
        each reflecting alternative methods for addressing the temporal reasoning 
        problem, resulting in different architectures for combining pre-trained models.
        Only open-set models were included in the evaluation due to their generality.
        <br><br>    
        <b>G<sup>2</sup>TR</b>: This model leverages a video-understanding VLM 
        <a href="https://arxiv.org/abs/2408.16500" class="external-link is-normal">[CogVLM2-video]</a>
        for candidate 
        interval identification, followed by target detection and grounding using a Vision 
        Language Model <a href="https://arxiv.org/abs/2303.08774" class="external-link is-normal">[GPT-4]</a> 
         and a Phrase Grounder 
         <a href="https://arxiv.org/abs/2405.10300" class="external-link is-normal">[Grounding DINO 1.5 Pro]</a>
         . It also integrates 
        a tracker module <a href="https://arxiv.org/abs/2408.00714" class="external-link is-normal">[SAM 2]</a>
         to handle environmental changes.

        <div>
          <img src="./static/gif/EvalCorpus/G2TR.gif" />
        </div>

        <br>
        <b>Direct Temporal Visual Grounding (DTVG)</b>: This approach uses a video-understanding VLM 
        for both temporal and spatial reasoning, followed by a Phrase Grounder to identify 
        the intended object, e.g., “green cloth on the left.”
        <br><br>
        <div></div>
          <img src="./static/gif/EvalCorpus/DTVG.gif" />
        </div>

        
        <b>Refined Temporal Visual Grounding (RTVG)</b>: Similar to DTVG, this method incorporates 
        Visual Question Answering (VQA) by the VLM to iteratively refine the object description, 
        ensuring precise grounding.

        <div></div>
          <img src="./static/gif/EvalCorpus/RTVG.gif" />
        </div>

      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experimental Insights</h2>
      <div id="tasks" , class="column has-text-justified">
        <p>
          We have compared the performance of the proposed
          model G<sup>2</sup>TR with alternative approaches 
          -Direct Temporal Visual Grounding (DTVG) and Refined Temporal Visual Grounding (RTVG).
          The proposed approach has better performance 
          than suggested alternate approaches by <b>26%</b>, in terms
          of <i>overall average grounding accuracy</i>. It also performs better 
          than the alternate approaches in each of the different scenarios 
          that are considered in our dataset bifurcation.
          <br><br>
          Analysis of our pipeline,G<sup>2</sup>TR , shows that the <b>main bottleneck </b>
          is the <i><b>Event Localizer’s</b></i> difficulty in precisely identifying the time instant
          of the language-referenced interaction. The next bottleneck
          occurs in the <i><b>Target Detector</b></i>, which struggles with selection
          of incorrect object class, insufficient visual prompts, or
          incorrect selection of object label due to ambiguity in visual
          options. The Tracker occasionally begins tracking the wrong
          object, when it appears identical to the correct one. 
          While our approach performs well in complex scenarios,
          it struggles with rapid human-object interactions and single
          actions involving multiple objects, such as replacing or
          stacking. Another limitation of our pipeline is its tendency
          to perform reasoning based on linguistic cues rather than
          visual evidence. For instance, when tasked with identifying
          the object into which water was poured, it may incorrectly
          output a cup, even if the water was poured into a different
          container.
        </p>
      </div>
      <hr>
      
      <div class="column is-centered is-flex is-flex-direction-column is-align-items-center">
        <div class="content has-text-centered">
          <h3 class="title is-4">Component-Wise Analysis of Failure Cases (45/155)</h3>
          <img src="static/gif/EvalCorpus/failure_case_wheel.gif" alt="GIF 4" style="width: 400px; height: 300px; border-radius: 25px;">
          <p class="teaser has-text-justified">
            Component-Wise Analysis frequency of failure cases generated by each module
          </p>
        </div>
      </div>
      


        <div class="column is-centered">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Failure Case Examples</h3>
            
            
              <!-- First Row of Images -->
              <div class="columns">
                <div class="column is-centered is-one-third">
                    <figure style="position: relative;">
                      <figcaption style="text-align: center; margin-bottom: 10px; font-size: 15px;"><i><b>Instruction: </b>Robot, pick bottle placed just before orange bottle</i></figcaption>
                      <img src="static/gif/EvalCorpus/failure_case.gif" alt="Image 1 Description" style="width:100%;">
                      <p style="text-align: center;"><b>(a) Incorrect time instant</b> (Expected: 5, G<sup>2</sup>TR: 12)</p>
                    </figure>
                </div>
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px; font-size: 15px;"><i><b>Instruction: </b>Pick the container in which water was poured</i></figcaption>
                      <img src="static/gif/EvalCorpus/failure_case2.gif" alt="Image 2 Description" style="width:100%;">
                      <p style="text-align: center;"><b>(b) Incorrect Object Class</b> (Expected: Bottle, G<sup>2</sup>TR: Cup)</p>
                    </figure>
                </div>
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px;font-size: 15px;"><i><b>Instruction: </b>Pick the container by which water was poured</i></figcaption>
                      <img src="static/images/failure_case_4.png" alt="Image 3 Description" style="width:100%; height:110 px">
                      <p style="text-align: center;"><b>(c) Visual Options Ambiguity</b> (Object labels overlapping)</p>
                    </figure>
                </div>
            </div>

            <!-- Second Row of Images -->
            <div class="columns">
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px;font-size: 15px;"><i><b>Instruction: </b>Robot, pick the cup under<br> which strawberry is hidden!</i></figcaption>
                      <img src="static/gif/EvalCorpus/tracking_lost.gif" alt="Image 4 Description" style="width:60%;">
                      <p style="text-align: center;"><b>(d) Incorrect Object Tracking</b><br> (Wrong cup tracked)</p>
                    </figure>
                </div>
                
                <div class="column">
                    <figure>
                      <figcaption style="text-align: center; margin-bottom: 10px;font-size: 15px;"><i><b>Instruction: </b>Robot, point to the cup which<br> was just used by the person</i></figcaption>
                      <img src="static/images/failure_case_5.png" alt="Image 6 Description" style="width:60%;">
                      <p style="text-align: center;"><b>(e) Insufficient Grounding</b> <br>(One cup not grounded)</p>
                    </figure>
                </div>
            </div>
  

                
              
              
            
            <div class="is-flex is-justify-content-center">
              <p class="teaser has-text-justified" style="max-width: 600px;">
                Examples of Failure Cases displays instances of failure within G<sup>2</sup>TR framework.
              </p>
            </div>
          </div>
        </div>

        </div> 
      </div>
    </div>
    
  </section>

  <section class="section", style="text-align: center;"></section>
    <div class="container is-max-desktop">
      <!-- Motivation. -->
      <h2 class="title has-text-centered is-3"> Additional Insights</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
        
        <b><i>a. Leveraging a combination of specialized temporal and spatial video and image-understanding models can provide 
          deeper contextual understanding compared to only a general-purpose image-understanding vision-language model.</i></b>
        <br>
        Interactions are inherently temporal, and videos capture the 
        full sequence, preserving context and flow. Converting this to the 
        image domain requires sub-sampling, which is challenging as the appropriate 
        rate depends on the complexity of the action. This can result in losing 
        crucial details, making video a more reliable medium for understanding interactions.
        <br><br>
        <b><i>b. Grounding propagation through semantic tracking enhances the G<sup>2</sup>TR's ability to maintain consistent 
          object identification across frames, even after initial detection, ensuring more robust performance in 
          dynamic environments.</i></b>
        <br>
        The relative position of the target object might change after the language-referenced interaction 
        took place. It is essential that its position be determined in the present world scene of the robot.
        <br><br>
        <b><i>c. Providing visual options for the image-understanding vision-language model enhances its adaptability by 
          allowing it to better handle ambiguity and improve decision-making in complex visual scenarios.</i></b>
        <br>
        Converting the spatial description of target object in linguistic description 
        might be difficult in visually complex scenarios and confuse the grounding model. 
        It is therefore relatively easier and more effective to ground the object class first 
        and then pick the target object from these grounded visual options.
          

  

      </div>
  </section>
 

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Conclusion</h2>
      <div class="column has-text-justified">
        <p>
        We present G<sup>2</sup>TR, a novel approach to
        grounded temporal reasoning. We factorize the problem into
        three key components: (i) candidate interval localization
        in a video based on required interactions, (ii) fine-grained
        spatial reasoning within the localized interval to ground the
        target object, and (iii) tracking the object post-interaction.
        By leveraging pre-trained visual language models and large
        language models, G<sup>2</sup>TR achieves zero-shot generalization for
        both object set and interactions. We also propose a dataset
        of 155 video-instruction pairs covering spatially complex,
        multi-hop, partially observable, and multi-interaction tempo-
        ral reasoning tasks. Evaluation on the dataset shows signif-
        icant improvement over alternative approaches, highlighting
        G<sup>2</sup>TR’s potential in robot instruction following. Finally, it
        is important to note that G2TR currently has two limits: (i)
        it can only process videos up to one minute, as constrained
        by the video-reasoning model, and (ii) it can ground only
        a single object at a time. We aim to overcome both these
        limitations in future.
        </p>
      </div>
      <hr>

      
    </div>
  </section>

  <hr>

  <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [CogVLM2-video] Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, 
        Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others
        CogVLM2: Visual Language Models for Image and Video Understanding
        arXiv preprint arXiv:2408.16500, 2024.</code></pre>
      <pre><code>2. [GPT-4] Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, 
        Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others.
        Gpt-4 technical report
        arXiv preprint arXiv:2303.08774, 2023.</code></pre>
      <pre><code>3. [Grounding DINO 1.5 Pro] Tianhe Ren and Qing Jiang and Shilong Liu and Zhaoyang Zeng and Wenlong Liu 
        and Han Gao and Hongjie Huang and Zhengyu Ma and Xiaoke Jiang and Yihao Chen and Yuda Xiong and Hao Zhang  
        and Feng Li and Peijun Tang and Kent Yu and Lei Zhang.
        Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection
        arXiv preprint arXiv:2405.10300v2, 2024.</code></pre>
      <pre><code>4. [SAM 2] Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and 
        Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, 
        Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, 
        Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph
        SAM 2: Segment Anything in Images and Videos
        arXiv preprint arXiv:2408.00714, 2024.</code></pre>
    </div>
  </section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{phyplan2024,
      title     = {PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators},
      author    = {Vagadia, Harshil and Chopra, Mudit and Barnawal, Abhinav and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
